{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334f975e-d82b-4570-8f14-10a0f5981d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14266428-4c9e-43fb-93c3-0421a38ec8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,bottleneck_dim),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def get_encoder_output(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72a1ca9-1b8a-42db-8ad9-1459ef9f5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load Word2Vec vectors from the JSON file\n",
    "with open('word_to_vector.json', 'r') as file:\n",
    "    word_vectors_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5758098-01f1-40a9-99c7-ddd3ea61ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vector size of input (embedding size of each word in Word2Vec model)\n",
    "input_dim = 400\n",
    "\n",
    "# Define the AutoEncoder model\n",
    "autoencoder = Autoencoder(input_dim=input_dim, bottleneck_dim=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c91225c2-ef1d-4bfe-a3ed-2148999763b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an appropriate loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8b75e1-b765-47ae-b38a-58c766bc5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3ed6be-c2ae-4cfd-882e-80630ab82b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Average Loss: 0.0300\n",
      "Epoch [2/3], Average Loss: 0.0417\n",
      "Epoch [3/3], Average Loss: 0.0360\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Iterate over all words in the loaded word vectors dictionary\n",
    "    for word, word_vector in word_vectors_dict.items():\n",
    "        # Convert the word vector to a PyTorch tensor\n",
    "        input_data = torch.tensor(word_vector, dtype=torch.float32).view(1, input_dim)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed_input = autoencoder(input_data)\n",
    "\n",
    "        # Calculate the loss (MSE between input and reconstructed output)\n",
    "        loss = criterion(reconstructed_input, input_data)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate and print the average loss for the epoch\n",
    "    average_loss = total_loss / len(word_vectors_dict)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b060126d-e072-4530-8794-02c781393c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new embeddings from the trained autoencoder\n",
    "new_embeddings = {}\n",
    "for word, word_vector in word_vectors_dict.items():\n",
    "    input_data = torch.tensor(word_vector, dtype=torch.float32).view(1, input_dim)\n",
    "    encoded_output = autoencoder.get_encoder_output(input_data).detach().numpy().tolist()\n",
    "    new_embeddings[word] = encoded_output[0]\n",
    "\n",
    "# Save the new embeddings to a new JSON file\n",
    "with open('reduced_word2vec_embeddings.json', 'w') as file:\n",
    "    json.dump(new_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14deccd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
